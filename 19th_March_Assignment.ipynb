{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application.\n",
        "Min-Max scaling, also known as normalization, is a technique used in data preprocessing to scale numeric features in a dataset to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all features have the same scale, preventing some features from dominating others during the training of machine learning models.\n",
        "\n",
        "The formula for Min-Max scaling is given by:\n",
        "\n",
        "\n",
        "\n",
        "X\n",
        "scaled\n",
        "​\n",
        " = X−X\n",
        "min/\n",
        "X\n",
        "max\n",
        "​\n",
        " −X\n",
        "min\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "1. X is the original value of the feature.\n",
        "�\n",
        "2.\n",
        "Xmin is the minimum value of the feature in the dataset.\n",
        "\n",
        "3. Xmax is the maximum value of the feature in the dataset.\n",
        "\n",
        "This transformation ensures that the scaled values fall within the [0, 1] range, with 0 corresponding to the minimum value in the original data and 1 corresponding to the maximum value.\n",
        "\n",
        "Here's an example in Python using the scikit-learn library:"
      ],
      "metadata": {
        "id": "zckj6hO2YFCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = np.array([[1.0, 2.0],\n",
        "                 [5.0, 6.0],\n",
        "                 [10.0, 12.0]])\n",
        "\n",
        "# Create a MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Original data:\\n\", data)\n",
        "print(\"\\nScaled data:\\n\", scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dNZnIVRaDtL",
        "outputId": "cf19a0f7-1e61-4d7d-f50b-b06d82c1393f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            " [[ 1.  2.]\n",
            " [ 5.  6.]\n",
            " [10. 12.]]\n",
            "\n",
            "Scaled data:\n",
            " [[0.         0.        ]\n",
            " [0.44444444 0.4       ]\n",
            " [1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?    \n",
        "Provide an example to illustrate its application.\n",
        "The Unit Vector technique in feature scaling, also known as vector normalization or unit normalization, is a method that scales the values of individual features in a dataset to ensure that each feature vector has a Euclidean length (L2 norm) of 1. This technique is commonly used in machine learning when the direction of the data points is more important than their magnitudes.\n",
        "\n",
        "The formula for unit vector scaling is given by:\n",
        "Xscaled = X/||X||   \n",
        "Where,   \n",
        "1. X  is the original feature vector.\n",
        "2. ||X|| is the Euclidean norm (length) of the feature vector, calculated as sq.root(∑i=1-n xi^2).\n",
        "Unit Vector scaling ensures that the resulting vectors lie on the surface of a unit hypersphere, which is a hypersphere with a radius of 1.\n",
        "\n",
        "Here's an example in Python using the scikit-learn library:\n",
        ""
      ],
      "metadata": {
        "id": "vJno9BJmYE8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = np.array([[1.0, 2.0],\n",
        "                 [5.0, 6.0],\n",
        "                 [10.0, 12.0]])\n",
        "\n",
        "# Create a Normalizer with L2 normalization (default)\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Transform the data\n",
        "normalized_data = normalizer.transform(data)\n",
        "\n",
        "print(\"Original data:\\n\", data)\n",
        "print(\"\\nNormalized data (unit vectors):\\n\", normalized_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3diG8tcb90G",
        "outputId": "ddaab7ad-f9cd-45e7-fa34-9726aec0b8af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            " [[ 1.  2.]\n",
            " [ 5.  6.]\n",
            " [10. 12.]]\n",
            "\n",
            "Normalized data (unit vectors):\n",
            " [[0.4472136  0.89442719]\n",
            " [0.6401844  0.76822128]\n",
            " [0.6401844  0.76822128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the Normalizer is used with the default L2 normalization (unit vector scaling). The transform method applies the normalization to each row (feature vector) in the data array. The resulting normalized_data will have feature vectors with a Euclidean length of 1.\n",
        "\n",
        "Differences from Min-Max Scaling:\n",
        "\n",
        "1. Min-Max scaling scales features to a specific range (usually [0, 1]), while Unit Vector scaling normalizes each feature vector to have a length of 1.\n",
        "2. Min-Max scaling is concerned with the overall scale of the data, whereas Unit Vector scaling is focused on the direction of the data vectors.\n",
        "3. Unit Vector scaling is often used when the magnitude of the features is less important than their relative relationships or directions."
      ],
      "metadata": {
        "id": "PF4APJ07YE3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application.\n",
        "\n",
        "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction and feature extraction. Its primary goal is to transform a dataset into a new coordinate system, where the axes correspond to the principal components of variation in the data. These principal components are linear combinations of the original features, and they are ordered by the amount of variance they explain.\n",
        "\n",
        "PCA works by identifying the directions (principal components) in which the data varies the most. The first principal component explains the most variance, the second explains the second most, and so on. By selecting a subset of the principal components that capture the majority of the variance, one can reduce the dimensionality of the dataset while retaining essential information.\n",
        "\n",
        "Here's a simple example using Python and scikit-learn:"
      ],
      "metadata": {
        "id": "dUGtONwvYEzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = np.array([[1.0, 2.0, 3.0],\n",
        "                 [4.0, 5.0, 6.0],\n",
        "                 [7.0, 8.0, 9.0]])\n",
        "\n",
        "# Create a PCA instance with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the data\n",
        "transformed_data = pca.fit_transform(data)\n",
        "\n",
        "# Print the original and transformed data\n",
        "print(\"Original data:\\n\", data)\n",
        "print(\"\\nTransformed data (2 principal components):\\n\", transformed_data)\n",
        "print(\"\\nExplained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKM5NY66mxsW",
        "outputId": "d9e2758e-2a6c-402c-e42c-7d9f78c4c138"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            " [[1. 2. 3.]\n",
            " [4. 5. 6.]\n",
            " [7. 8. 9.]]\n",
            "\n",
            "Transformed data (2 principal components):\n",
            " [[-5.19615242e+00  2.56395025e-16]\n",
            " [ 0.00000000e+00  0.00000000e+00]\n",
            " [ 5.19615242e+00  2.56395025e-16]]\n",
            "\n",
            "Explained variance ratio: [1.00000000e+00 2.43475588e-33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the PCA class from scikit-learn is used to perform dimensionality reduction. The n_components parameter is set to 2, indicating that we want to reduce the data to 2 dimensions. The fit_transform method is then applied to the original data, providing the transformed data in the new coordinate system.\n",
        "\n",
        "The explained_variance_ratio_ attribute shows the proportion of variance explained by each principal component. In practice, you may choose the number of components based on a desired level of explained variance (e.g., 95% or 99%).\n",
        "\n",
        "PCA is particularly useful for reducing the dimensionality of datasets with a large number of features, improving computational efficiency and potentially mitigating issues like overfitting."
      ],
      "metadata": {
        "id": "U-r4N7PLYEut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept.\n",
        "\n",
        "PCA (Principal Component Analysis) is a technique commonly used for both dimensionality reduction and feature extraction. In the context of feature extraction, PCA helps identify the most important features or combinations of features in a dataset by transforming the original features into a new set of uncorrelated variables called principal components. These principal components represent patterns of variation in the data.\n",
        "\n",
        "The relationship between PCA and feature extraction lies in the fact that the principal components can be considered as new features that capture the most significant information in the original data. The goal is to reduce the dimensionality of the dataset while retaining as much relevant information as possible.\n",
        "\n",
        "Here's an example in Python using scikit-learn to demonstrate how PCA can be used for feature extraction:"
      ],
      "metadata": {
        "id": "Z-oFjEODYEnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = np.array([[1.0, 2.0, 3.0],\n",
        "                 [4.0, 5.0, 6.0],\n",
        "                 [7.0, 8.0, 9.0]])\n",
        "\n",
        "# Create a PCA instance with the same number of components as features\n",
        "pca = PCA(n_components=data.shape[1])\n",
        "\n",
        "# Fit and transform the data\n",
        "transformed_data = pca.fit_transform(data)\n",
        "\n",
        "# Print the original and transformed data\n",
        "print(\"Original data:\\n\", data)\n",
        "print(\"\\nTransformed data (principal components as features):\\n\", transformed_data)\n",
        "print(\"\\nExplained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeCpwglsnyzr",
        "outputId": "723c3fe2-9b23-48b3-b055-dfe0d7e554d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            " [[1. 2. 3.]\n",
            " [4. 5. 6.]\n",
            " [7. 8. 9.]]\n",
            "\n",
            "Transformed data (principal components as features):\n",
            " [[-5.19615242e+00  2.56395025e-16  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 5.19615242e+00  2.56395025e-16  0.00000000e+00]]\n",
            "\n",
            "Explained variance ratio: [1.00000000e+00 2.43475588e-33 0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the n_components parameter is set to the number of original features, indicating that we want to retain all possible principal components. The fit_transform method is then applied to the original data, producing a transformed dataset with principal components as features.\n",
        "\n",
        "The explained_variance_ratio_ attribute shows the proportion of variance explained by each principal component. You can use this information to decide how many principal components to retain based on the desired level of explained variance.\n",
        "\n",
        "In practice, the new set of principal components can be used as features for subsequent analysis or modeling tasks, providing a more compact representation of the data while preserving its essential information. Feature extraction using PCA is especially valuable when dealing with high-dimensional datasets and can contribute to better interpretability, reduced computational complexity, and improved model performance."
      ],
      "metadata": {
        "id": "SFxmWU4-YEYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data.\n",
        "Here's how I would use Min-Max scaling to preprocess the data for your food delivery service recommendation system:\n",
        "\n",
        "1. Identify features to scale:\n",
        "\n",
        "Price, rating, and delivery time are all numerical features and therefore suitable for Min-Max scaling.\n",
        "Categorical features (like cuisine type) shouldn't be scaled and require different preprocessing techniques.\n",
        "2. Define the desired range:\n",
        "\n",
        "Typically, Min-Max scaling is done to a range of 0-1. However, consider adjusting the range based on your model or purpose.\n",
        "For instance, if ratings are already on a 1-5 scale, it might be better to leave them as is.\n",
        "3. Apply Min-Max scaling:\n",
        "\n",
        "Use a library like scikit-learn's MinMaxScaler to fit the scaler on the training data.\n",
        "This calculates the minimum and maximum values for each feature.\n",
        "Apply the fitted scaler to transform both the training and test/validation data.\n",
        "Benefits of Min-Max scaling in this scenario:\n",
        "\n",
        "Improved algorithm performance: Recommendation models often rely on distance metrics, and features on similar scales contribute more equally to recommendations.\n",
        "Enhanced interpretability: Scaled features are easier to compare and understand their relative importance within the model.\n",
        "Prevents bias: Features with larger ranges won't dominate recommendations due to their scale.\n",
        "Considerations:\n",
        "\n",
        "Outliers: Min-Max scaling is sensitive to outliers, which can distort the scaling. Consider outlier detection and treatment techniques beforehand.\n",
        "Feature selection: Not all features might be relevant for recommendations. Feature selection techniques can help improve model performance.\n",
        "Alternative scaling methods: Depending on your model and data distribution, other scaling techniques like standardization might be more suitable.\n",
        "Additionally:\n",
        "\n",
        "Preprocess other features appropriately, using techniques like one-hot encoding for categorical features.\n",
        "Split the data into training, validation, and test sets before scaling to avoid data leakage.\n",
        "By carefully applying Min-Max scaling and other preprocessing techniques, you can prepare your food delivery data for a more effective and accurate recommendation system."
      ],
      "metadata": {
        "id": "tBopWobLoCwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
        "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
        "dimensionality of the dataset.\n",
        "\n",
        "Using PCA for Dimensionality Reduction in Stock Price Prediction:\n",
        "In your stock price prediction project with a high-dimensional dataset, Principal Component Analysis (PCA) can be a valuable tool for dimensionality reduction. Here's how I would use it:\n",
        "\n",
        "1. Data Preprocessing:\n",
        "\n",
        "Cleaning and Missing Value Imputation: Ensure data cleanliness by addressing missing values and outliers.\n",
        "Standardization: Standardize all features to have zero mean and unit variance. This makes PCA calculations more accurate.\n",
        "2. Applying PCA:\n",
        "\n",
        "Fit PCA Model: Train a PCA model on the preprocessed data. This calculates the principal components (PCs), which are linear combinations of the original features explaining the most variance.\n",
        "Determine the Number of Components: Decide how many PCs to retain. Usually, you aim to capture a significant portion of the variance (e.g., 80-90%) with as few components as possible. You can use the \"elbow method\" or scree plot to visualize the variance explained by each PC.\n",
        "3. Dimensionality Reduction:\n",
        "\n",
        "Project Data onto PCs: Transform the original data by projecting it onto the retained PCs. This results in a new dataset with fewer dimensions (i.e., the retained PCs).\n",
        "Benefits of using PCA for stock price prediction:\n",
        "\n",
        "Improved Model Performance: Reduces overfitting risk in models with many features.\n",
        "Reduced Training Time and Complexity: Lower-dimensional data leads to faster training and simpler models.\n",
        "Feature Interpretability: PCs can identify underlying relationships and patterns in the data.\n",
        "Considerations:\n",
        "\n",
        "Loss of Information: Reducing dimensions discards some information. Choose the number of PCs carefully to balance information loss with dimensionality reduction benefits.\n",
        "Non-linear Relationships: PCA primarily captures linear relationships. It might not be ideal if non-linear relationships are crucial for prediction."
      ],
      "metadata": {
        "id": "kQv6UuKwuXRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
        "values to a range of -1 to 1.\n",
        "\n",
        "Here are the scaled values after applying Min-Max scaling to the dataset [1, 5, 10, 15, 20] to a range of -1 to 1:\n",
        "\n",
        "[-1.0, -0.5789, -0.0526, 0.4737, 1.0]\n",
        "\n",
        "Here's the breakdown of the scaling process:\n",
        "\n",
        "Find the minimum and maximum values:\n",
        "\n",
        "Minimum value: 1\n",
        "Maximum value: 20\n",
        "Apply the Min-Max scaling formula to each value:\n",
        "\n",
        "scaled_value = (value - min_value) / (max_value - min_value) * 2 - 1\n",
        "For example, to scale the value 5:\n",
        "\n",
        "scaled_value = (5 - 1) / (20 - 1) * 2 - 1 = -0.5789\n",
        "\n",
        "Key points:\n",
        "\n",
        "The original range of values (1 to 20) has been transformed to a range of -1 to 1.\n",
        "The relative distances between the values remain preserved.\n",
        "Min-Max scaling is often used to prepare data for machine learning algorithms that are sensitive to feature magnitudes."
      ],
      "metadata": {
        "id": "pl4s-NgYv0b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
        "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "\n",
        "Feature Extraction using PCA for the given dataset:\n",
        "\n",
        "1. Data Preprocessing:\n",
        "\n",
        "Encode categorical features: Convert gender into numerical values using techniques like one-hot encoding.\n",
        "Standardize features: Standardize all numerical features (height, weight, age, blood pressure) to have zero mean and unit variance. This ensures PCA calculations are not biased by different scales.\n",
        "2. Apply PCA:\n",
        "\n",
        "Fit the PCA model: Train a PCA model on the preprocessed data.\n",
        "Calculate explained variance: Determine the proportion of variance explained by each principal component (PC).\n",
        "3. Choose Number of Components:\n",
        "\n",
        "Visualize variance: Create a scree plot or use the \"elbow method\" to visualize the explained variance and identify a suitable number of PCs.\n",
        "Consider domain knowledge: Incorporate domain expertise to assess the interpretability and relevance of PCs.\n",
        "Balance information loss and complexity: Aim to capture a significant portion of the variance (e.g., 80-90%) while reducing dimensionality effectively.\n",
        "Decision-Making:\n",
        "\n",
        "Based on the scree plot: If a clear \"elbow\" indicates a substantial drop in explained variance after a certain number of PCs, retain those PCs.\n",
        "Consider interpretability: If PCs have meaningful interpretations (e.g., representing body size), consider retaining them for better model understanding.\n",
        "Experiment with different numbers: Evaluate model performance with various PC choices to determine the optimal balance.\n",
        "Additional Considerations:\n",
        "\n",
        "Feature interactions: PCA might not capture complex non-linear relationships between features. Consider alternative dimensionality reduction techniques if such relationships are crucial.\n",
        "Validation: Always evaluate the impact of PCA on model performance using a validation set to avoid overfitting.\n",
        "Specific Guidance for Retaining Components:\n",
        "\n",
        "Without the actual dataset: I cannot definitively determine the optimal number of PCs. However, I'll provide a process-driven approach to guide the decision:\n",
        "\n",
        "Visualize the explained variance: Plot the cumulative explained variance against the number of PCs.\n",
        "Consider the \"elbow\" point: Look for a sharp decrease in the slope of the curve, indicating diminishing returns in variance capture.\n",
        "Evaluate interpretability: Assess the meaningfulness of the PCs based on domain knowledge.\n",
        "Experiment with different numbers: Try different PC choices in your model to observe performance variations."
      ],
      "metadata": {
        "id": "s4tPI9SfybPH"
      }
    }
  ]
}